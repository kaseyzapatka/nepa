{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Generation Capacity Extraction\n",
    "\n",
    "This notebook helps you:\n",
    "1. See real examples of capacity mentions in documents\n",
    "2. Understand which document types contain capacity info\n",
    "3. Test different extraction approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd().parent\n",
    "PROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "ANALYSIS_DIR = BASE_DIR / \"data\" / \"analysis\"\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Clean Energy Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load projects\n",
    "projects = pd.read_parquet(ANALYSIS_DIR / \"projects_combined.parquet\")\n",
    "clean_projects = projects[projects['project_energy_type'] == 'Clean'].copy()\n",
    "\n",
    "print(f\"Total projects: {len(projects):,}\")\n",
    "print(f\"Clean energy projects: {len(clean_projects):,}\")\n",
    "print()\n",
    "print(\"Clean energy by dataset source:\")\n",
    "print(clean_projects['dataset_source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Capacity Search Terms\n",
    "\n",
    "We'll use a set of terms to quickly filter sentences that *might* contain capacity information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms that indicate generation capacity might be mentioned\n",
    "CAPACITY_TERMS = {\n",
    "    # Units\n",
    "    'mw', 'gw', 'kw', 'mwh', 'gwh', 'kwh',\n",
    "    'megawatt', 'megawatts', 'gigawatt', 'gigawatts', \n",
    "    'kilowatt', 'kilowatts',\n",
    "    # Context words\n",
    "    'nameplate', 'capacity', 'generate', 'generating', \n",
    "    'generation', 'output', 'rated'\n",
    "}\n",
    "\n",
    "def has_capacity_terms(text):\n",
    "    \"\"\"Fast check using set intersection.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return False\n",
    "    words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "    return bool(words & CAPACITY_TERMS)\n",
    "\n",
    "def extract_candidate_sentences(text, context_chars=50):\n",
    "    \"\"\"Extract sentences that might contain capacity info.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Split into sentences (roughly)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    candidates = []\n",
    "    for sent in sentences:\n",
    "        if has_capacity_terms(sent) and len(sent) > 30:\n",
    "            candidates.append(sent.strip())\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Some EIS Projects (Most Likely to Have Capacity)\n",
    "\n",
    "EIS documents are for major projects - they should have capacity info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clean energy EIS projects (most likely to have capacity info)\n",
    "eis_clean = clean_projects[clean_projects['dataset_source'] == 'EIS']\n",
    "print(f\"Clean energy EIS projects: {len(eis_clean):,}\")\n",
    "print()\n",
    "print(\"Project types in EIS:\")\n",
    "print(eis_clean['project_type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EIS documents metadata\n",
    "eis_docs = pd.read_parquet(PROCESSED_DIR / \"eis\" / \"documents.parquet\")\n",
    "\n",
    "# Clean project_id if needed\n",
    "def extract_id(x):\n",
    "    if isinstance(x, dict):\n",
    "        return x.get('value', '')\n",
    "    return x\n",
    "\n",
    "eis_docs['project_id'] = eis_docs['project_id'].apply(extract_id)\n",
    "\n",
    "print(f\"EIS documents: {len(eis_docs):,}\")\n",
    "print()\n",
    "print(\"Document types:\")\n",
    "print(eis_docs['document_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Look at Specific Projects\n",
    "\n",
    "Let's pick a few solar/wind projects and examine their documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find solar projects in EIS\n",
    "solar_projects = eis_clean[eis_clean['project_type'].str.contains('solar', case=False, na=False)]\n",
    "wind_projects = eis_clean[eis_clean['project_type'].str.contains('wind', case=False, na=False)]\n",
    "\n",
    "print(f\"Solar EIS projects: {len(solar_projects)}\")\n",
    "print(f\"Wind EIS projects: {len(wind_projects)}\")\n",
    "\n",
    "# Show a few\n",
    "print(\"\\n=== Sample Solar Projects ===\")\n",
    "solar_projects[['project_id', 'project_title', 'project_type']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one project to examine in detail\n",
    "if len(solar_projects) > 0:\n",
    "    sample_project = solar_projects.iloc[0]\n",
    "    project_id = sample_project['project_id']\n",
    "    \n",
    "    print(f\"Project: {sample_project['project_title']}\")\n",
    "    print(f\"ID: {project_id}\")\n",
    "    print(f\"Type: {sample_project['project_type']}\")\n",
    "    print(f\"Location: {sample_project.get('project_location', 'N/A')}\")\n",
    "else:\n",
    "    # Fallback to any EIS clean project\n",
    "    sample_project = eis_clean.iloc[0]\n",
    "    project_id = sample_project['project_id']\n",
    "    print(f\"Using project: {sample_project['project_title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get documents for this project\n",
    "project_docs = eis_docs[eis_docs['project_id'] == project_id]\n",
    "print(f\"Documents for this project: {len(project_docs)}\")\n",
    "print()\n",
    "project_docs[['document_id', 'document_type', 'document_title', 'main_document', 'total_pages']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Read Pages and Find Capacity Mentions\n",
    "\n",
    "Now let's load pages for this project's main document and search for capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the main document (preferably FEIS or DEIS)\n",
    "main_docs = project_docs[project_docs['main_document'] == 'YES']\n",
    "if len(main_docs) == 0:\n",
    "    main_docs = project_docs\n",
    "\n",
    "# Prioritize FEIS > DEIS > EA\n",
    "type_priority = {'FEIS': 1, 'DEIS': 2, 'EA': 3, 'FONSI': 4, '': 5, 'OTHER': 6}\n",
    "main_docs = main_docs.copy()\n",
    "main_docs['_priority'] = main_docs['document_type'].map(lambda x: type_priority.get(x, 10))\n",
    "main_docs = main_docs.sort_values('_priority')\n",
    "\n",
    "target_doc = main_docs.iloc[0]\n",
    "target_doc_id = target_doc['document_id']\n",
    "\n",
    "print(f\"Target document: {target_doc['document_title']}\")\n",
    "print(f\"Type: {target_doc['document_type']}\")\n",
    "print(f\"Pages: {target_doc['total_pages']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pages for this document only (using pyarrow filter)\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pages_path = PROCESSED_DIR / \"eis\" / \"pages.parquet\"\n",
    "\n",
    "# Read with filter to avoid loading all 6M pages\n",
    "pages_table = pq.read_table(\n",
    "    pages_path, \n",
    "    filters=[('document_id', '=', target_doc_id)]\n",
    ")\n",
    "doc_pages = pages_table.to_pandas()\n",
    "\n",
    "print(f\"Loaded {len(doc_pages)} pages for document {target_doc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for capacity mentions in first 30 pages\n",
    "capacity_findings = []\n",
    "\n",
    "for _, page in doc_pages.head(30).iterrows():\n",
    "    page_num = page['page_number']\n",
    "    text = page['page_text']\n",
    "    \n",
    "    candidates = extract_candidate_sentences(text)\n",
    "    \n",
    "    for sent in candidates:\n",
    "        # Look for actual numbers near capacity terms\n",
    "        if re.search(r'\\d+\\s*(?:MW|GW|kW|megawatt|gigawatt)', sent, re.IGNORECASE):\n",
    "            capacity_findings.append({\n",
    "                'page': page_num,\n",
    "                'sentence': sent[:300]  # Truncate long sentences\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(capacity_findings)} sentences mentioning capacity with numbers\\n\")\n",
    "\n",
    "for i, finding in enumerate(capacity_findings[:10]):\n",
    "    print(f\"--- Page {finding['page']} ---\")\n",
    "    print(finding['sentence'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test on Multiple Projects\n",
    "\n",
    "Let's scan several projects to see the variety of capacity mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_capacity_sentences_for_project(project_id, docs_df, pages_path, max_pages=20):\n",
    "    \"\"\"Find capacity-related sentences for a project.\"\"\"\n",
    "    # Get project's documents\n",
    "    project_docs = docs_df[docs_df['project_id'] == project_id]\n",
    "    if project_docs.empty:\n",
    "        return []\n",
    "    \n",
    "    # Prioritize main documents\n",
    "    main_docs = project_docs[project_docs['main_document'] == 'YES']\n",
    "    if main_docs.empty:\n",
    "        main_docs = project_docs\n",
    "    \n",
    "    # Get first main document\n",
    "    doc_id = main_docs.iloc[0]['document_id']\n",
    "    \n",
    "    # Load pages\n",
    "    try:\n",
    "        pages_table = pq.read_table(pages_path, filters=[('document_id', '=', doc_id)])\n",
    "        doc_pages = pages_table.to_pandas()\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    # Search first N pages\n",
    "    findings = []\n",
    "    for _, page in doc_pages.head(max_pages).iterrows():\n",
    "        candidates = extract_candidate_sentences(page['page_text'])\n",
    "        for sent in candidates:\n",
    "            if re.search(r'\\d+\\s*(?:MW|GW|kW|megawatt|gigawatt)', sent, re.IGNORECASE):\n",
    "                findings.append({\n",
    "                    'page': page['page_number'],\n",
    "                    'sentence': sent[:400]\n",
    "                })\n",
    "    \n",
    "    return findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10 clean energy EIS projects\n",
    "sample_projects = eis_clean.sample(min(10, len(eis_clean)), random_state=42)\n",
    "\n",
    "print(\"Scanning sample projects for capacity mentions...\\n\")\n",
    "\n",
    "for _, proj in sample_projects.iterrows():\n",
    "    findings = find_capacity_sentences_for_project(\n",
    "        proj['project_id'], \n",
    "        eis_docs, \n",
    "        pages_path\n",
    "    )\n",
    "    \n",
    "    print(f\"=== {proj['project_title'][:60]}... ===\")\n",
    "    print(f\"Type: {proj['project_type']}\")\n",
    "    print(f\"Found {len(findings)} capacity mentions\")\n",
    "    \n",
    "    if findings:\n",
    "        # Show first finding\n",
    "        print(f\"Example (page {findings[0]['page']}): {findings[0]['sentence'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Coverage Analysis\n",
    "\n",
    "What percentage of projects have capacity mentions in their first 20 pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on a larger sample to estimate coverage\n",
    "# WARNING: This may take a few minutes\n",
    "\n",
    "sample_size = 50  # Adjust as needed\n",
    "sample = eis_clean.sample(min(sample_size, len(eis_clean)), random_state=123)\n",
    "\n",
    "results = []\n",
    "for i, (_, proj) in enumerate(sample.iterrows()):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing {i+1}/{len(sample)}...\")\n",
    "    \n",
    "    findings = find_capacity_sentences_for_project(\n",
    "        proj['project_id'], \n",
    "        eis_docs, \n",
    "        pages_path,\n",
    "        max_pages=20\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'project_id': proj['project_id'],\n",
    "        'project_title': proj['project_title'],\n",
    "        'project_type': proj['project_type'],\n",
    "        'has_capacity_mention': len(findings) > 0,\n",
    "        'num_mentions': len(findings),\n",
    "        'first_sentence': findings[0]['sentence'][:200] if findings else None\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Coverage Summary ===\")\n",
    "print(f\"Projects with capacity mentions: {results_df['has_capacity_mention'].sum()} / {len(results_df)}\")\n",
    "print(f\"Coverage rate: {results_df['has_capacity_mention'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show projects WITHOUT capacity mentions - these need investigation\n",
    "no_capacity = results_df[~results_df['has_capacity_mention']]\n",
    "print(f\"\\nProjects without capacity mentions ({len(no_capacity)}):\")\n",
    "no_capacity[['project_title', 'project_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example: What LLM Input Would Look Like\n",
    "\n",
    "If we send candidate sentences to an LLM, here's what the input/output would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example LLM prompt construction\n",
    "def build_llm_prompt(sentences, project_title):\n",
    "    \"\"\"Build a prompt for LLM extraction.\"\"\"\n",
    "    prompt = f\"\"\"Extract the proposed project's generation capacity from the following text excerpts.\n",
    "\n",
    "Project: {project_title}\n",
    "\n",
    "Return JSON with this structure:\n",
    "{{\n",
    "    \"capacity_value\": <number or null>,\n",
    "    \"capacity_unit\": \"MW\" | \"GW\" | \"kW\" | null,\n",
    "    \"confidence\": \"high\" | \"medium\" | \"low\",\n",
    "    \"source_quote\": \"<exact quote from text>\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Extract the PRIMARY project capacity, not alternatives or comparisons\n",
    "- If multiple values exist, prefer \"nameplate\" or \"rated\" capacity\n",
    "- If text mentions a range (e.g., \"50-100 MW\"), use the higher value\n",
    "- Return null values if no clear project capacity is stated\n",
    "\n",
    "Text excerpts:\n",
    "\"\"\"\n",
    "    for i, sent in enumerate(sentences[:5]):  # Limit to 5 sentences\n",
    "        prompt += f\"\\n[{i+1}] {sent}\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Show example\n",
    "if len(capacity_findings) > 0:\n",
    "    example_sentences = [f['sentence'] for f in capacity_findings[:5]]\n",
    "    example_prompt = build_llm_prompt(example_sentences, sample_project['project_title'])\n",
    "    print(\"=== Example LLM Prompt ===\")\n",
    "    print(example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare EA vs EIS vs CE Coverage\n",
    "\n",
    "Let's see which document types have the most capacity mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on EA documents\n",
    "ea_clean = clean_projects[clean_projects['dataset_source'] == 'EA']\n",
    "ea_docs = pd.read_parquet(PROCESSED_DIR / \"ea\" / \"documents.parquet\")\n",
    "ea_docs['project_id'] = ea_docs['project_id'].apply(extract_id)\n",
    "\n",
    "print(f\"Clean energy EA projects: {len(ea_clean)}\")\n",
    "\n",
    "# Sample and check\n",
    "ea_sample = ea_clean.sample(min(20, len(ea_clean)), random_state=456)\n",
    "ea_pages_path = PROCESSED_DIR / \"ea\" / \"pages.parquet\"\n",
    "\n",
    "ea_results = []\n",
    "for _, proj in ea_sample.iterrows():\n",
    "    findings = find_capacity_sentences_for_project(\n",
    "        proj['project_id'], \n",
    "        ea_docs, \n",
    "        ea_pages_path,\n",
    "        max_pages=20\n",
    "    )\n",
    "    ea_results.append({'has_capacity': len(findings) > 0})\n",
    "\n",
    "ea_coverage = sum(r['has_capacity'] for r in ea_results) / len(ea_results) * 100\n",
    "print(f\"EA coverage estimate: {ea_coverage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on CE documents (expect lower coverage)\n",
    "ce_clean = clean_projects[clean_projects['dataset_source'] == 'CE']\n",
    "ce_docs = pd.read_parquet(PROCESSED_DIR / \"ce\" / \"documents.parquet\")\n",
    "ce_docs['project_id'] = ce_docs['project_id'].apply(extract_id)\n",
    "\n",
    "print(f\"Clean energy CE projects: {len(ce_clean)}\")\n",
    "\n",
    "# Sample and check\n",
    "ce_sample = ce_clean.sample(min(20, len(ce_clean)), random_state=789)\n",
    "ce_pages_path = PROCESSED_DIR / \"ce\" / \"pages.parquet\"\n",
    "\n",
    "ce_results = []\n",
    "for _, proj in ce_sample.iterrows():\n",
    "    findings = find_capacity_sentences_for_project(\n",
    "        proj['project_id'], \n",
    "        ce_docs, \n",
    "        ce_pages_path,\n",
    "        max_pages=20\n",
    "    )\n",
    "    ce_results.append({'has_capacity': len(findings) > 0})\n",
    "\n",
    "ce_coverage = sum(r['has_capacity'] for r in ce_results) / len(ce_results) * 100\n",
    "print(f\"CE coverage estimate: {ce_coverage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile coverage estimates\n",
    "coverage_summary = pd.DataFrame([\n",
    "    {'source': 'EIS', 'clean_projects': len(eis_clean), 'estimated_coverage': f\"{results_df['has_capacity_mention'].mean()*100:.1f}%\" if len(results_df) > 0 else 'N/A'},\n",
    "    {'source': 'EA', 'clean_projects': len(ea_clean), 'estimated_coverage': f\"{ea_coverage:.1f}%\" if 'ea_coverage' in dir() else 'N/A'},\n",
    "    {'source': 'CE', 'clean_projects': len(ce_clean), 'estimated_coverage': f\"{ce_coverage:.1f}%\" if 'ce_coverage' in dir() else 'N/A'},\n",
    "])\n",
    "\n",
    "print(\"=== Coverage Summary by Document Source ===\")\n",
    "print(coverage_summary.to_string(index=False))\n",
    "print()\n",
    "print(\"Note: Coverage = % of projects with MW/GW/kW mentions in first 20 pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on this exploration:\n",
    "\n",
    "1. **Prioritize EIS and EA documents** - they have higher coverage rates\n",
    "2. **Use sentence filtering** - the set-based approach finds relevant text quickly\n",
    "3. **Consider LLM for extraction** - the filtered sentences are good input for structured extraction\n",
    "4. **Focus on first 20 pages** - capacity is usually mentioned early in project descriptions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
